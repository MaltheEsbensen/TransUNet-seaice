{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb031557-1c83-407f-ac01-91664e53d95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add proprietary modules into the path\n",
    "#from pathlib import Path\n",
    "#sys.path.append(str(Path.home().joinpath(\"AI4ArcticSeaIceChallenge/\")))\n",
    "\n",
    "# -- Built-in modules -- #\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# -- Environmental variables -- #\n",
    "os.environ['AI4ARCTIC_DATA'] = '/Volumes/T7/21316608'  # Fill in directory for data location.\n",
    "os.environ['AI4ARCTIC_ENV'] = '/Users/mepercuter/Desktop/AI4ArcticSeaIceChallenge-main/'  # Fill in directory for environment with Ai4Arctic get-started package. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82d24f0-233b-41f9-95ef-af0cc0895800",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- Third-part modules -- #\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import xarray as xr\n",
    "from tqdm.notebook import tqdm  # Progress bar\n",
    "import math\n",
    "\n",
    "# --Proprietary modules -- #\n",
    "from functions import chart_cbar, r2_metric, f1_metric, compute_metrics  # Functions to calculate metrics and show the relevant chart colorbar.\n",
    "from loaders import AI4ArcticChallengeDataset, AI4ArcticChallengeTestDataset, get_variable_options  # Custom dataloaders for regular training and validation.\n",
    "from AutoIceTransUNet import UNetViT #from unettrans import UNet  # Convolutional Neural Network model\n",
    "from utils import CHARTS, SIC_LOOKUP, SOD_LOOKUP, FLOE_LOOKUP, SCENE_VARIABLES, colour_str\n",
    "\n",
    "#Define crop size\n",
    "crop_size = 512\n",
    "\n",
    "train_options = {\n",
    "    # -- Training options -- #\n",
    "    #'path_to_processed_data': os.environ['AI4ARCTIC_DATA'],\n",
    "    'path_to_processed_data': os.environ['AI4ARCTIC_DATA'],  # Replace with data directory path.\n",
    "    'path_to_env': os.environ['AI4ARCTIC_ENV'],  # Replace with environmment directory path.\n",
    "    'lr': 0.0001,  # Optimizer learning rate.\n",
    "    'epochs': 120,  # Number of epochs before training stop.\n",
    "    'epoch_len': 100,  # Number of batches for each epoch.\n",
    "    'patch_size': int(crop_size),  # Size of patches sampled. Used for both Width and Height.\n",
    "    'batch_size': 24,  # Number of patches for each batch.\n",
    "    'loader_upsampling': 'nearest',  # How to upscale low resolution variables to high resolution.\n",
    "    \n",
    "    # -- Data prepraration lookups and metrics.\n",
    "    'train_variables': SCENE_VARIABLES,  # Contains the relevant variables in the scenes.\n",
    "    'charts': CHARTS,  # Charts to train on.\n",
    "    'n_classes': {  # number of total classes in the reference charts, including the mask.\n",
    "        'SIC': SIC_LOOKUP['n_classes'],\n",
    "        'SOD': SOD_LOOKUP['n_classes'],\n",
    "        'FLOE': FLOE_LOOKUP['n_classes']\n",
    "    },\n",
    "    \n",
    "    'pixel_spacing': 80,  # SAR pixel spacing. 80 for the ready-to-train AI4Arctic Challenge dataset.\n",
    "    'train_fill_value': 0,  # Mask value for SAR training data.\n",
    "    'class_fill_values': {  # Mask value for class/reference data.\n",
    "        'SIC': SIC_LOOKUP['mask'],\n",
    "        'SOD': SOD_LOOKUP['mask'],\n",
    "        'FLOE': FLOE_LOOKUP['mask'],\n",
    "    },\n",
    "    \n",
    "    # -- Validation options -- #\n",
    "    'chart_metric': {  # Metric functions for each ice parameter and the associated weight.\n",
    "        'SIC': {\n",
    "            'func': r2_metric,\n",
    "            'weight': 2,\n",
    "        },\n",
    "        'SOD': {\n",
    "            'func': f1_metric,\n",
    "            'weight': 2,\n",
    "        },\n",
    "        'FLOE': {\n",
    "            'func': f1_metric,\n",
    "            'weight': 1,\n",
    "        },\n",
    "    },\n",
    "    'num_val_scenes': 30,  # Number of scenes randomly sampled from train_list to use in validation.\n",
    "    \n",
    "    # -- GPU/cuda options -- #\n",
    "    'gpu_id': 0,  # Index of GPU. In case of multiple GPUs.\n",
    "    'num_workers': 6,  # Number of parallel processes to fetch data.\n",
    "    'num_workers_val': 1,  # Number of parallel processes during validation.\n",
    "    \n",
    "    # -- U-Net Options -- #\n",
    "    'unet_conv_filters': [16, 32, 64, 128],  # Number of filters in the U-Net.\n",
    "    'conv_kernel_size': (3, 3),  # Size of convolutional kernels.\n",
    "    'conv_stride_rate': (1, 1),  # Stride rate of convolutional kernels.\n",
    "    'conv_dilation_rate': (1, 1),  # Dilation rate of convolutional kernels.\n",
    "    'conv_padding': (1, 1),  # Number of padded pixels in convolutional layers.\n",
    "    'conv_padding_style': 'zeros',  # Style of padding.\n",
    "}\n",
    "# Get options for variables, amsrenv grid, cropping and upsampling.\n",
    "get_variable_options = get_variable_options(train_options)\n",
    "# To be used in test_upload.\n",
    "%store train_options  \n",
    "\n",
    "# Load training list.\n",
    "#with open('AI4ArcticSeaIceChallenge/datalists/dataset.json') as file:\n",
    "#    train_options['train_list'] = json.loads(file.read())\n",
    "with open(train_options['path_to_env'] + 'datalists/dataset.json') as file:\n",
    "    train_options['train_list'] = json.loads(file.read())\n",
    "# Convert the original scene names to the preprocessed names.\n",
    "train_options['train_list'] = [file[17:32] + '_' + file[77:80] + '_prep.nc' for file in train_options['train_list']]\n",
    "# Select a random number of validation scenes with the same seed. Feel free to change the seed.et\n",
    "np.random.seed(0)\n",
    "train_options['validate_list'] = np.random.choice(np.array(train_options['train_list']), size=train_options['num_val_scenes'], replace=False)\n",
    "# Remove the validation scenes from the train list.\n",
    "train_options['train_list'] = [scene for scene in train_options['train_list'] if scene not in train_options['validate_list']]\n",
    "train_options['test_list'] = np.random.choice(np.array(train_options['train_list']), size=20, replace=False)\n",
    "train_options['test_list1'] = np.random.choice(np.array(train_options['test_list']), size=10, replace=False)\n",
    "# Remove the validation scenes from the train list.\n",
    "train_options['train_list'] = [scene for scene in train_options['train_list'] if scene not in train_options['test_list']]\n",
    "# Remove the validation scenes from the train list.\n",
    "train_options['test_list'] = [scene for scene in train_options['test_list'] if scene not in train_options['test_list1']]\n",
    "print(len(train_options['train_list']))\n",
    "print(len(train_options['validate_list']))\n",
    "print(len(train_options['test_list']))\n",
    "print(len(train_options['test_list1']))\n",
    "print(len(train_options['charts']))\n",
    "print('Options initialised')\n",
    "\n",
    "print('Setup ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c3c79a-3f60-4ca3-a6e9-b967929a3c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GPU resources.\n",
    "if torch.cuda.is_available():\n",
    "    print(colour_str('GPU available!', 'green'))\n",
    "    print('Total number of available devices: ', colour_str(torch.cuda.device_count(), 'orange'))\n",
    "    device = torch.device(f\"cuda:{train_options['gpu_id']}\")\n",
    "\n",
    "else:\n",
    "    print(colour_str('GPU not available.', 'red'))\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Custom dataset and dataloader.\n",
    "dataset = AI4ArcticChallengeDataset(files=train_options['train_list'], options=train_options)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=None, shuffle=True, num_workers=train_options['num_workers'], pin_memory=True)\n",
    "# - Setup of the validation dataset/dataloader. The same is used for model testing in 'test_upload.ipynb'.\n",
    "dataset_val = AI4ArcticChallengeTestDataset(options=train_options, files=train_options['validate_list'])\n",
    "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=None, num_workers=train_options['num_workers_val'], shuffle=False)\n",
    "# - Setup of the test dataset/dataloader. The same is used for model testing in 'test_upload.ipynb'.\n",
    "dataset_test = AI4ArcticChallengeTestDataset(options=train_options, files=train_options['test_list'])\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=None, num_workers=train_options['num_workers_val'], shuffle=False)\n",
    "# - Setup of the test dataset/dataloader. The same is used for model testing in 'test_upload.ipynb'.\n",
    "dataset_test1 = AI4ArcticChallengeTestDataset(options=train_options, files=train_options['test_list1'])\n",
    "dataloader_test1 = torch.utils.data.DataLoader(dataset_test1, batch_size=None, num_workers=train_options['num_workers_val'], shuffle=False)\n",
    "\n",
    "print('GPU and data setup complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df663da8-9779-4f8e-b641-29c9ea4e6036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup U-Net model, adam optimizer, loss function and dataloader.\n",
    "#UNetViT(in_channels=24, num_classes=64).to(device) #\n",
    "model = UNetViT(in_channels=24, num_classes=16).to(device) #net = UNet(options=train_options).to(device)\n",
    "optimizer = torch.optim.Adam(list(model.parameters()), lr=train_options['lr'])\n",
    "torch.backends.cudnn.benchmark = True  # Selects the kernel with the best performance for the GPU and given input size.\n",
    "\n",
    "# Loss functions to use for each sea ice parameter.\n",
    "# The ignore_index argument discounts the masked values, ensuring that the model is not using these pixels to train on.\n",
    "# It is equivalent to multiplying the loss of the relevant masked pixel with 0.\n",
    "loss_functions = {chart: torch.nn.CrossEntropyLoss(ignore_index=train_options['class_fill_values'][chart]) \\\n",
    "                                                   for chart in train_options['charts']}\n",
    "#model.load_state_dict(torch.load('best_model', map_location=torch.device('cpu'))['model_state_dict'])\n",
    "print('Model setup complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7add9631-46da-49db-89e5-722ea9acd5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load('best_model', map_location=torch.device('cpu'))['model_state_dict'])\n",
    "Test_folder = 'testtest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a96e0c7-5799-4fc4-b9d5-3bc3a9babc24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Testing 1.')\n",
    "\n",
    "os.makedirs(f\"{Test_folder}\", exist_ok=True)\n",
    "# - Stores the output and the reference pixels to calculate the scores after inference on all the scenes.\n",
    "outputs_flat = {chart: np.array([]) for chart in train_options['charts']}\n",
    "inf_ys_flat = {chart: np.array([]) for chart in train_options['charts']}\n",
    "\n",
    "model.eval()  # Set network to evaluation mode.\n",
    "i=0\n",
    "# - Loops though scenes in queue.\n",
    "for inf_x, inf_y, masks, name in tqdm(iterable=asid_loader, total=len(train_options['test_list']), colour='green', position=0):\n",
    "    scene_name = name[:19]  # Removes the _prep.nc from the name.\n",
    "    torch.cuda.empty_cache()\n",
    "    inf_x = inf_x.to(device, non_blocking=True)\n",
    "\n",
    "    # - Ensures that no gradients are calculated, which otherwise take up a lot of space on the GPU.\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        output = model(inf_x)\n",
    "\n",
    "    # - Final output layer, and storing of non masked pixels.\n",
    "    for chart in train_options['charts']:\n",
    "        output[chart] = torch.argmax(output[chart], dim=1).squeeze().cpu().numpy()\n",
    "        outputs_flat[chart] = np.append(outputs_flat[chart], output[chart][~masks[chart]])\n",
    "        inf_y[chart] = inf_y[chart].squeeze()\n",
    "        inf_ys_flat[chart] = np.append(inf_ys_flat[chart], inf_y[chart][~masks[chart]].numpy())\n",
    "        inf_y[chart] = inf_y[chart].cpu().numpy()\n",
    "        \n",
    "    inf_x = inf_x.squeeze()\n",
    "    inf_x = inf_x.cpu().numpy()\n",
    "    inf_x1 = inf_x\n",
    "    \n",
    "    del inf_x\n",
    "        \n",
    "    # - Show the scene inference.\n",
    "    fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(10, 10))\n",
    "    for idx, chart in enumerate(train_options['charts']):\n",
    "        ax = axs[0,idx]\n",
    "        output[chart] = output[chart].astype(float)\n",
    "        output[chart][masks[chart]] = np.nan\n",
    "        ax.imshow(output[chart], vmin=0, vmax=train_options['n_classes'][chart] - 2, cmap='jet', interpolation='nearest')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        chart_cbar(ax=ax, n_classes=train_options['n_classes'][chart], chart=chart, cmap='jet')\n",
    "        ax1 = axs[1,idx]\n",
    "        inf_y[chart] = inf_y[chart].astype(float)\n",
    "        inf_y[chart][masks[chart]] = np.nan\n",
    "        ax1.imshow(inf_y[chart], vmin=0, vmax=train_options['n_classes'][chart] - 2, cmap='jet', interpolation='nearest')\n",
    "        ax1.set_xticks([])\n",
    "        ax1.set_yticks([])\n",
    "        chart_cbar(ax=ax1, n_classes=train_options['n_classes'][chart], chart=chart, cmap='jet')\n",
    "    ax2 = axs[2,0]\n",
    "    inf_x1[0] = inf_x1[0].astype(float)\n",
    "    ax2.imshow(inf_x1[0], cmap='gray')\n",
    "    ax2.set_xticks([])\n",
    "    ax2.set_yticks([])\n",
    "    ax3 = axs[2,1]\n",
    "    inf_x1[1] = inf_x1[1].astype(float)\n",
    "    ax3.imshow(inf_x1[1], cmap='gray')\n",
    "    ax3.set_xticks([])\n",
    "    ax3.set_yticks([])\n",
    "    ax4 = axs[2,2]\n",
    "    inf_x1[3] = inf_x1[3].astype(float)\n",
    "    ax4.imshow(inf_x1[3], cmap='gray')\n",
    "    ax4.set_xticks([])\n",
    "    ax4.set_yticks([])\n",
    "    \n",
    "    plt.suptitle(f\"SIC, SoD and FLOE predictions. Scene: {scene_name}\", y=1)\n",
    "    plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0.5, hspace=-0)\n",
    "    fig.savefig(f\"{Test_folder}/{scene_name}.png\", format='png', dpi=128, bbox_inches=\"tight\")\n",
    "    plt.close('all')\n",
    "\n",
    "    del inf_x1, inf_y, masks, output, fig, axs, ax, ax1, ax2, ax3, ax4  # Free memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f829866",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix_SIC = confusion_matrix(inf_ys_flat['SIC'], outputs_flat['SIC'])\n",
    "\n",
    "SIC_GROUPS = ('0','10','20','30','40','50','60','70','80','90','100')\n",
    "SOD_GROUPS = ('Open water','New Ice','Young ice','Thin FYI','Thick FYI','Old ice')\n",
    "FLOE_GROUPS = ('Open water','Cake Ice','Small floe','Medium floe','Big floe','Vast floe','Bergs')\n",
    "\n",
    "cf_matrix_SOD = confusion_matrix(inf_ys_flat['SOD'], outputs_flat['SOD'])\n",
    "\n",
    "\n",
    "cf_matrix_FLOE = confusion_matrix(inf_ys_flat['FLOE'], outputs_flat['FLOE'])\n",
    "\n",
    "# - Compute the relevant scores.\n",
    "print('Calculating Metrics for Testing')\n",
    "combined_score, scores = compute_metrics(true=inf_ys_flat, pred=outputs_flat, charts=train_options['charts'],\n",
    "                                         metrics=train_options['chart_metric'])\n",
    "\n",
    "print('Metrics Calculated for Testing')\n",
    "\n",
    "print(\"\")\n",
    "for chart in train_options['charts']:\n",
    "    print(f\"{chart} {train_options['chart_metric'][chart]['func'].__name__}: {scores[chart]}%\")\n",
    "print(f\"Combined score: {combined_score}%\")\n",
    "del inf_ys_flat, outputs_flat  # Free memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56ee3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing 2.')\n",
    "# - Stores the output and the reference pixels to calculate the scores after inference on all the scenes.\n",
    "outputs_flat = {chart: np.array([]) for chart in train_options['charts']}\n",
    "inf_ys_flat = {chart: np.array([]) for chart in train_options['charts']}\n",
    "\n",
    "model.eval()  # Set network to evaluation mode.\n",
    "i=0\n",
    "# - Loops though scenes in queue.\n",
    "for inf_x, inf_y, masks, name in tqdm(iterable=asid_loader1, total=len(train_options['test_list1']), colour='green', position=0):\n",
    "    scene_name = name[:19]  # Removes the _prep.nc from the name.\n",
    "    torch.cuda.empty_cache()\n",
    "    inf_x = inf_x.to(device, non_blocking=True)\n",
    "\n",
    "    # - Ensures that no gradients are calculated, which otherwise take up a lot of space on the GPU.\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        output = model(inf_x)\n",
    "\n",
    "    # - Final output layer, and storing of non masked pixels.\n",
    "    for chart in train_options['charts']:\n",
    "        output[chart] = torch.argmax(output[chart], dim=1).squeeze().cpu().numpy()\n",
    "        outputs_flat[chart] = np.append(outputs_flat[chart], output[chart][~masks[chart]])\n",
    "        inf_y[chart] = inf_y[chart].squeeze()\n",
    "        inf_ys_flat[chart] = np.append(inf_ys_flat[chart], inf_y[chart][~masks[chart]].numpy())\n",
    "        inf_y[chart] = inf_y[chart].cpu().numpy()\n",
    "        \n",
    "    inf_x = inf_x.squeeze()\n",
    "    inf_x = inf_x.cpu().numpy()\n",
    "    inf_x1 = inf_x\n",
    "    \n",
    "    del inf_x\n",
    "        \n",
    "    # - Show the scene inference.\n",
    "    fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(10, 10))\n",
    "    for idx, chart in enumerate(train_options['charts']):\n",
    "        ax = axs[0,idx]\n",
    "        output[chart] = output[chart].astype(float)\n",
    "        output[chart][masks[chart]] = np.nan\n",
    "        ax.imshow(output[chart], vmin=0, vmax=train_options['n_classes'][chart] - 2, cmap='jet', interpolation='nearest')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        chart_cbar(ax=ax, n_classes=train_options['n_classes'][chart], chart=chart, cmap='jet')\n",
    "        ax1 = axs[1,idx]\n",
    "        inf_y[chart] = inf_y[chart].astype(float)\n",
    "        inf_y[chart][masks[chart]] = np.nan\n",
    "        ax1.imshow(inf_y[chart], vmin=0, vmax=train_options['n_classes'][chart] - 2, cmap='jet', interpolation='nearest')\n",
    "        ax1.set_xticks([])\n",
    "        ax1.set_yticks([])\n",
    "        chart_cbar(ax=ax1, n_classes=train_options['n_classes'][chart], chart=chart, cmap='jet')\n",
    "    ax2 = axs[2,0]\n",
    "    inf_x1[0] = inf_x1[0].astype(float)\n",
    "    ax2.imshow(inf_x1[0], cmap='gray')\n",
    "    ax2.set_xticks([])\n",
    "    ax2.set_yticks([])\n",
    "    ax3 = axs[2,1]\n",
    "    inf_x1[1] = inf_x1[1].astype(float)\n",
    "    ax3.imshow(inf_x1[1], cmap='gray')\n",
    "    ax3.set_xticks([])\n",
    "    ax3.set_yticks([])\n",
    "    ax4 = axs[2,2]\n",
    "    inf_x1[3] = inf_x1[3].astype(float)\n",
    "    ax4.imshow(inf_x1[3], cmap='gray')\n",
    "    ax4.set_xticks([])\n",
    "    ax4.set_yticks([])\n",
    "    \n",
    "    plt.suptitle(f\"SIC, SoD and FLOE predictions. Scene: {scene_name}\", y=1)\n",
    "    plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0.5, hspace=-0)\n",
    "    fig.savefig(f\"{Test_folder}/{scene_name}.png\", format='png', dpi=128, bbox_inches=\"tight\")\n",
    "    plt.close('all')\n",
    "\n",
    "    del inf_x1, inf_y, masks, output, fig, axs, ax, ax1, ax2, ax3, ax4  # Free memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cfc969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix_SIC1 = confusion_matrix(inf_ys_flat['SIC'], outputs_flat['SIC'])\n",
    "\n",
    "SIC_GROUPS = ('0','10','20','30','40','50','60','70','80','90','100')\n",
    "SOD_GROUPS = ('Open water','New Ice','Young ice','Thin FYI','Thick FYI','Old ice')\n",
    "FLOE_GROUPS = ('Open water','Cake Ice','Small floe','Medium floe','Big floe','Vast floe','Bergs')\n",
    "\n",
    "# Build confusion matrix\n",
    "df_cm_SIC = pd.DataFrame((cf_matrix_SIC+cf_matrix_SIC1) / np.sum((cf_matrix_SIC+cf_matrix_SIC1), axis=1)[:, None], index = [i for i in SIC_GROUPS],\n",
    "                     columns = [i for i in SIC_GROUPS])\n",
    "plt.figure(figsize = (12,7))\n",
    "sn.heatmap(df_cm_SIC, annot=True, fmt='.2%', cmap='Blues')\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.savefig(f'{Test_folder}/output1_SIC.png')\n",
    "\n",
    "\n",
    "cf_matrix_SOD1 = confusion_matrix(inf_ys_flat['SOD'], outputs_flat['SOD'])\n",
    "df_cm_SOD = pd.DataFrame((cf_matrix_SOD+cf_matrix_SOD1) / np.sum((cf_matrix_SOD+cf_matrix_SOD1), axis=1)[:, None], index = [i for i in SOD_GROUPS],\n",
    "                     columns = [i for i in SOD_GROUPS])\n",
    "plt.figure(figsize = (12,7))\n",
    "sn.heatmap(df_cm_SOD, annot=True, fmt='.2%', cmap='Blues')\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.savefig(f'{Test_folder}/output1_SOD.png')\n",
    "\n",
    "\n",
    "cf_matrix_FLOE1 = confusion_matrix(inf_ys_flat['FLOE'], outputs_flat['FLOE'])\n",
    "df_cm_FLOE = pd.DataFrame((cf_matrix_FLOE+cf_matrix_FLOE1) / np.sum((cf_matrix_FLOE+cf_matrix_FLOE1), axis=1)[:, None], index = [i for i in FLOE_GROUPS],\n",
    "                     columns = [i for i in FLOE_GROUPS])\n",
    "plt.figure(figsize = (12,7))\n",
    "sn.heatmap(df_cm_FLOE, annot=True, fmt='.2%', cmap='Blues')\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.savefig(f'{Test_folder}/output_FLOE.png')\n",
    "\n",
    "# - Compute the relevant scores.\n",
    "print('Calculating Metrics for Testing')\n",
    "combined_score, scores = compute_metrics(true=inf_ys_flat, pred=outputs_flat, charts=train_options['charts'],\n",
    "                                         metrics=train_options['chart_metric'])\n",
    "\n",
    "print('Metrics Calculated for Testing')\n",
    "\n",
    "print(\"\")\n",
    "for chart in train_options['charts']:\n",
    "    print(f\"{chart} {train_options['chart_metric'][chart]['func'].__name__}: {scores[chart]}%\")\n",
    "print(f\"Combined score: {combined_score}%\")\n",
    "del inf_ys_flat, outputs_flat  # Free memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ce3f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_combined_score = 0  # Best weighted model score.\n",
    "learn_rate = train_options['lr']#0.0001#\n",
    "# -- Training Loop -- #\n",
    "for epoch in tqdm(iterable=range(train_options['epochs']), position=0):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(list(model.parameters()), lr=learn_rate)\n",
    "    gc.collect()  # Collect garbage to free memory.\n",
    "    loss_sum = torch.tensor([0.])  # To sum the batch losses during the epoch.\n",
    "    model.train()  # Set network to evaluation mode.\n",
    "\n",
    "    # Loops though batches in queue.\n",
    "    for i, (batch_x, batch_y) in enumerate(tqdm(iterable=dataloader, total=train_options['epoch_len'], colour='red', position=0)):\n",
    "        torch.cuda.empty_cache()  # Empties the GPU cache freeing up memory.\n",
    "        loss_batch = 0  # Reset from previous batch.\n",
    "        \n",
    "        # - Transfer to device.\n",
    "        batch_x = batch_x.to(device, non_blocking=True)\n",
    "\n",
    "        # - Mixed precision training. (Saving memory)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(batch_x)\n",
    "            # - Calculate loss.\n",
    "            for chart in train_options['charts']:\n",
    "                loss_batch += loss_functions[chart](input=output[chart], target=batch_y[chart].to(device))\n",
    "\n",
    "        # - Reset gradients from previous pass.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # - Backward pass.\n",
    "        loss_batch.backward()\n",
    "\n",
    "        # - Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # - Add batch loss.\n",
    "        loss_sum += loss_batch.detach().item()\n",
    "\n",
    "        # - Average loss for displaying\n",
    "        loss_epoch = torch.true_divide(loss_sum, i + 1).detach().item()\n",
    "        print('\\rMean training loss: ' + f'{loss_epoch:.3f}', end='\\r')\n",
    "        del output, batch_x, batch_y # Free memory.\n",
    "    del loss_sum\n",
    "    \n",
    "    # -- Validation Loop -- #\n",
    "    loss_batch = loss_batch.detach().item()  # For printing after the validation loop.\n",
    "    \n",
    "    # - Stores the output and the reference pixels to calculate the scores after inference on all the scenes.\n",
    "    outputs_flat = {chart: np.array([]) for chart in train_options['charts']}\n",
    "    inf_ys_flat = {chart: np.array([]) for chart in train_options['charts']}\n",
    "\n",
    "    model.eval()  # Set network to evaluation mode.\n",
    "    # - Loops though scenes in queue.\n",
    "    loss_val = 0\n",
    "    for inf_x, inf_y, masks, name in tqdm(iterable=dataloader_val, total=len(train_options['validate_list']), colour='green', position=0):\n",
    "        scene_name = name[:19]  # Removes the _prep.nc from the name.\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # - Ensures that no gradients are calculated, which otherwise take up a lot of space on the GPU.\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            inf_x = inf_x.to(device, non_blocking=True)\n",
    "            output = model(inf_x)\n",
    "    \n",
    "        # - Final output layer, and storing of non masked pixels.\n",
    "        for chart in train_options['charts']:\n",
    "            inf_y[chart] = inf_y[chart].unsqueeze(0)\n",
    "            loss_val += loss_functions[chart](input=output[chart], target=inf_y[chart].long().to(device))\n",
    "            output[chart] = torch.argmax(output[chart], dim=1).squeeze().cpu().numpy()\n",
    "            inf_y[chart] = inf_y[chart].squeeze()\n",
    "            outputs_flat[chart] = np.append(outputs_flat[chart], output[chart][~masks[chart]])\n",
    "            inf_ys_flat[chart] = np.append(inf_ys_flat[chart], inf_y[chart][~masks[chart]].numpy())\n",
    "            \n",
    "        del inf_x, inf_y, masks, output  # Free memory.\n",
    "\n",
    "    # - Compute the relevant scores.\n",
    "    print('Calculating Metrics')\n",
    "    combined_score, scores = compute_metrics(true=inf_ys_flat, pred=outputs_flat, charts=train_options['charts'],\n",
    "                                             metrics=train_options['chart_metric'])\n",
    "    \n",
    "    print('Metrics Calculated')\n",
    "\n",
    "    print(\"\")\n",
    "    print(f\"Final batch loss: {loss_batch:.3f}\")\n",
    "    print(f\"Epoch {epoch} score:\")\n",
    "    print(f\"Validation loss: {loss_val/(train_options['num_val_scenes']):.3f}\")\n",
    "    for chart in train_options['charts']:\n",
    "        print(f\"{chart} {train_options['chart_metric'][chart]['func'].__name__}: {scores[chart]}%\")\n",
    "    print(f\"Combined score: {combined_score}%\")\n",
    "\n",
    "    # If the scores is better than the previous epoch, then save the model and rename the image to best_validation.\n",
    "    if combined_score > best_combined_score:\n",
    "        best_combined_score = combined_score\n",
    "        torch.save(obj={'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'epoch': epoch},\n",
    "                        f='best_model')\n",
    "    del inf_ys_flat, outputs_flat  # Free memory.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "mlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "properties": {
   "name": "AutoICE - Quickstart"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
